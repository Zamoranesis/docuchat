{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHzB1Dn556L2"
      },
      "source": [
        "### 0)Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following notebook in Google Colab with GPU + High Ram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud0MNNhP56_O",
        "outputId": "6ff12b3d-4db1-4d55-fb30-57af151302d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiDXUyv656L7"
      },
      "outputs": [],
      "source": [
        "# # Restart kernel afterwards\n",
        "# # GPU llama-cpp-python\n",
        "# ! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "# # For downloading  models\n",
        "# ! pip install -q huggingface_hub\n",
        "# ! pip install -q ctransformers==0.2.5\n",
        "# ! pip install -q faiss-cpu==1.7.4\n",
        "# ! pip install -q langchain==0.0.225\n",
        "# ! pip install -q pypdf==3.8.1\n",
        "# ! pip install -q sentence-transformers==2.2.2\n",
        "# ! pip install -q uvicorn>=0.22.0\n",
        "# ! pip install -q gradio\n",
        "# ! pip install -q pymupdf==1.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-sJVcirW56L8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/Data_Science/NLP/LLM/docuchat/src\"\n",
        "os.chdir(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UCylD0PV56L9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f-qAclh656L9"
      },
      "outputs": [],
      "source": [
        "model_dir = \"../models\"\n",
        "model_id = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "file_name = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QhLYjCi656L-",
        "outputId": "5821dd82-9e08-440d-e951-21bbff4e6cbc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'../models'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEgC7aoa56L_"
      },
      "source": [
        "### 1)Load Model from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MUc386En56L_"
      },
      "outputs": [],
      "source": [
        "# Download the model into local store\n",
        "# https://huggingface.co/docs/huggingface_hub/guides/download\n",
        "model_path = hf_hub_download(repo_id=model_id,\n",
        "                            filename=file_name,\n",
        "                            local_dir = model_dir,\n",
        "                            local_dir_use_symlinks= True, # defines how the file must be saved in your local folder->False: File will be downloaded and moved directly to the local dir\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JGAN6GKE7SvS",
        "outputId": "ac61f0a8-ddd1-4bff-8c8c-7245376fd401"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'../models/llama-2-13b-chat.ggmlv3.q5_1.bin'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydjVjwQF699x"
      },
      "source": [
        "### 2)Run DocuChat App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqYclsU7B8l",
        "outputId": "52ee006d-46ee-4e1a-af81-04210b467089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Data_Science/NLP/LLM/docuchat/src/app.py:81: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  chatbot = gr.Chatbot(value=[],\n",
            "/content/drive/MyDrive/Data_Science/NLP/LLM/docuchat/src/app.py:84: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  show_img = gr.Image(label='Upload File',\n",
            "/content/drive/MyDrive/Data_Science/NLP/LLM/docuchat/src/app.py:90: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  txt = gr.Textbox(\n",
            "/content/drive/MyDrive/Data_Science/NLP/LLM/docuchat/src/app.py:101: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  ).style()\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://9661738261293edfb8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "Downloading (…)c7362/.gitattributes: 100% 1.18k/1.18k [00:00<00:00, 5.68MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100% 190/190 [00:00<00:00, 1.14MB/s]\n",
            "Downloading (…)de792c7362/README.md: 100% 4.51k/4.51k [00:00<00:00, 22.6MB/s]\n",
            "Downloading (…)792c7362/config.json: 100% 701/701 [00:00<00:00, 4.29MB/s]\n",
            "Downloading (…)ce_transformers.json: 100% 123/123 [00:00<00:00, 804kB/s]\n",
            "Downloading (…)c7362/eval/readme.md: 100% 4.00/4.00 [00:00<00:00, 24.2kB/s]\n",
            "Downloading (…)_sts-dev_results.csv: 100% 770/770 [00:00<00:00, 4.45MB/s]\n",
            "Downloading pytorch_model.bin: 100% 439M/439M [00:01<00:00, 301MB/s]\n",
            "Downloading (…)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 409kB/s]\n",
            "Downloading (…)-mt-test_results.csv: 100% 299/299 [00:00<00:00, 2.23MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 837kB/s]\n",
            "Downloading (…)c7362/tokenizer.json: 100% 480k/480k [00:00<00:00, 59.4MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 556/556 [00:00<00:00, 3.33MB/s]\n",
            "Downloading (…)de792c7362/vocab.txt: 100% 242k/242k [00:00<00:00, 46.9MB/s]\n",
            "Downloading (…)92c7362/modules.json: 100% 229/229 [00:00<00:00, 1.69MB/s]\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5\n",
            "llama.cpp: loading model from ../models/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 1028\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_head_kv  = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: n_gqa      = 1\n",
            "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: freq_base  = 10000.0\n",
            "llama_model_load_internal: freq_scale = 1\n",
            "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
            "llama_model_load_internal: using CUDA for GPU acceleration\n",
            "llama_model_load_internal: mem required  =  526.30 MB (+  803.12 MB per state)\n",
            "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 401 MB VRAM for the scratch buffer\n",
            "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
            "llama_model_load_internal: offloading non-repeating layers to GPU\n",
            "llama_model_load_internal: offloading v cache to GPU\n",
            "llama_model_load_internal: offloading k cache to GPU\n",
            "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
            "llama_model_load_internal: total VRAM used: 10398 MB\n",
            "llama_new_context_with_model: kv self size  =  803.12 MB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =   121.10 ms /   120 runs   (    1.01 ms per token,   990.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =  5247.36 ms /   543 tokens (    9.66 ms per token,   103.48 tokens per second)\n",
            "llama_print_timings:        eval time =  6747.82 ms /   119 runs   (   56.70 ms per token,    17.64 tokens per second)\n",
            "llama_print_timings:       total time = 12420.90 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =    45.89 ms /    47 runs   (    0.98 ms per token,  1024.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =  2230.97 ms /   210 tokens (   10.62 ms per token,    94.13 tokens per second)\n",
            "llama_print_timings:        eval time =  2640.95 ms /    46 runs   (   57.41 ms per token,    17.42 tokens per second)\n",
            "llama_print_timings:       total time =  5029.09 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =   174.75 ms /   177 runs   (    0.99 ms per token,  1012.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =  5741.64 ms /   591 tokens (    9.72 ms per token,   102.93 tokens per second)\n",
            "llama_print_timings:        eval time = 11626.07 ms /   176 runs   (   66.06 ms per token,    15.14 tokens per second)\n",
            "llama_print_timings:       total time = 17971.16 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =    22.75 ms /    23 runs   (    0.99 ms per token,  1010.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =  4251.86 ms /   415 tokens (   10.25 ms per token,    97.60 tokens per second)\n",
            "llama_print_timings:        eval time =  1263.90 ms /    22 runs   (   57.45 ms per token,    17.41 tokens per second)\n",
            "llama_print_timings:       total time =  5593.03 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =   271.79 ms /   276 runs   (    0.98 ms per token,  1015.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =  5871.38 ms /   607 tokens (    9.67 ms per token,   103.38 tokens per second)\n",
            "llama_print_timings:        eval time = 18861.58 ms /   275 runs   (   68.59 ms per token,    14.58 tokens per second)\n",
            "llama_print_timings:       total time = 25675.71 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =    39.38 ms /    40 runs   (    0.98 ms per token,  1015.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =  7004.47 ms /   706 tokens (    9.92 ms per token,   100.79 tokens per second)\n",
            "llama_print_timings:        eval time =  2871.53 ms /    39 runs   (   73.63 ms per token,    13.58 tokens per second)\n",
            "llama_print_timings:       total time = 10013.34 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  4489.10 ms\n",
            "llama_print_timings:      sample time =    96.05 ms /    96 runs   (    1.00 ms per token,   999.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =  6107.61 ms /   556 tokens (   10.98 ms per token,    91.03 tokens per second)\n",
            "llama_print_timings:        eval time =  7114.20 ms /    95 runs   (   74.89 ms per token,    13.35 tokens per second)\n",
            "llama_print_timings:       total time = 13555.46 ms\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2198, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Data_Science/NLP/LLM/docuchat/src/app.py\", line 133, in <module>\n",
            "    demo.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2114, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2202, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 49, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9661738261293edfb8.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! python app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWRYo8OP7f38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
